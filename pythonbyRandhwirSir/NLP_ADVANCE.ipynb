{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advance Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Tokenization\n",
    "''' Word tokenization\n",
    "    Sentence tokenization\n",
    "    Sub word(n-gram tokenizatioon)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'have', 'reached', 'fid', 'step']\n",
      "[('Tokenization', 'is', 'an'), ('is', 'an', 'impprt'), ('an', 'impprt', 'NLP'), ('impprt', 'NLP', 'task.It'), ('NLP', 'task.It', 'helps'), ('task.It', 'helps', 'break')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nisha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Apply Tokenization\n",
    "''' Word tokenization\n",
    "    Sentence tokenization\n",
    "    Sub word(n-gram tokenizatioon)\n",
    "'''\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import ngrams\n",
    "s1 = \"He have reached fid step\"\n",
    "t = word_tokenize(s1)\n",
    "print(t)\n",
    "s3 = \"Tokenization is an impprt NLP task.It helps break\"\n",
    "ng = list(ngrams((word_tokenize(s3)),n=3)) # it's return combination\n",
    "print(ng)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nisha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing stop words\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words(\"english\")\n",
    "len(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['our',\n",
       " 'team',\n",
       " 'name',\n",
       " 'is',\n",
       " 'learn',\n",
       " 'and',\n",
       " 'Build',\n",
       " 'and',\n",
       " 'we',\n",
       " 'have',\n",
       " 'selected']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s4=\"our team name is learn and Build and we have selected\"\n",
    "swws = [word for word in word_tokenize(s4)]\n",
    "swws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply Stemming\n",
    "        Porter stemmer\n",
    "        Snowball Stemmer\n",
    "        Lancaster Stemmer\n",
    "        Regexp stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''Porter Stemmer: Is the original stemmer but the stem sometime illogical or non dictionary word'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['coonect', 'connect', 'connectiong', 'connect', 'connect', 'connect']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "p=PorterStemmer()\n",
    "s5=\"Coonect Connection Connectiong Connected Connects Connectings\"\n",
    "ps = [p.stem(word) for word in word_tokenize(s5)]\n",
    "ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'am',\n",
       " 'here',\n",
       " 'to',\n",
       " 'connect',\n",
       " 'with',\n",
       " 'connect',\n",
       " 'i',\n",
       " 'got',\n",
       " '.',\n",
       " 'but',\n",
       " 'i',\n",
       " 'ca',\n",
       " \"n't\",\n",
       " 'connect',\n",
       " 'with',\n",
       " 'those',\n",
       " 'peopl']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#snowball stemmer:it is faster and more logical than porter stemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "sb = SnowballStemmer(language=\"english\")\n",
    "s6=\"I am here to connect with connections i got . But i can't connected with those people\"\n",
    "sbs=[sb.stem(word) for word in word_tokenize(s6)]\n",
    "sbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Lancaster stemmer: More aggressive and dynamic\n",
    "from ntlk.stem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'am',\n",
       " 'her',\n",
       " 'to',\n",
       " 'connect',\n",
       " 'with',\n",
       " 'connect',\n",
       " 'i',\n",
       " 'got',\n",
       " '.',\n",
       " 'but',\n",
       " 'i',\n",
       " 'ca',\n",
       " \"n't\",\n",
       " 'connect',\n",
       " 'with',\n",
       " 'thos',\n",
       " 'peopl']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "ls = LancasterStemmer()\n",
    "s6=\"I am here to connect with connections i got . But i can't connected with those people\"\n",
    "sbs=[ls.stem(word) for word in word_tokenize(s6)]\n",
    "sbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car', 'univers', 'fair', 'easy', 'sing', 'sing', 'sung', 'sing', 'sport']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "ls = LancasterStemmer()\n",
    "#s6=\"I am here to connect with connections i got . But i can't connected with those people\"\n",
    "words = ['cared','university','fairly','easily','singing',\n",
    "       'sings','sung','singer','sportingly']\n",
    "sbs=[ls.stem(word) for word in words]\n",
    "sbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cared',\n",
       " 'university',\n",
       " 'fair',\n",
       " 'easi',\n",
       " 'sing',\n",
       " 'sing',\n",
       " 'sung',\n",
       " 'singer',\n",
       " 'sporting']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "rg = RegexpStemmer(regexp=\"ing$|s$|e$\\|tive$|ly$\")\n",
    "words = ['cared','university','fairly','easily','singing',\n",
    "       'sings','sung','singer','sportingly']\n",
    "sbs=[rg.stem(word) for word in words]\n",
    "sbs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date 21/04/2024 ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nisha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'here', 'to', 'connect', 'with', 'connection']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply lemmatization\n",
    "'''Wordnet Lemmeter'''\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import contractions\n",
    "nltk.download('wordnet')\n",
    "lemma = WordNetLemmatizer()\n",
    "s1 = \"I am here to connect with connection\"\n",
    "s1 = contractions.fix(s1)\n",
    "sl = [lemma.lemmatize(word,'n') for word in word_tokenize(s1) ]\n",
    "sl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'bat', 'are', 'hanging', 'on', 'their', 'foot', 'in']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob ,Word\n",
    "s1=\"The bats are hanging on their feet in \"\n",
    "s1 = TextBlob(s1)\n",
    "tb = [w.lemmatize() for w in s1.words]\n",
    "tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bat are hanging on their foot in\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob ,Word\n",
    "s1=\"The bats are hanging on their feet in \"\n",
    "s1 = TextBlob(s1)\n",
    "tb = [w.lemmatize() for w in s1.words]\n",
    "print(\" \".join(tb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More advance technique\n",
    "POS TAGGING\n",
    "NER TAGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\nisha\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: What || POS Tag: WP\n",
      "Word: is || POS Tag: VBZ\n",
      "Word: the || POS Tag: DT\n",
      "Word: step || POS Tag: NN\n",
      "Word: by || POS Tag: IN\n",
      "Word: step || POS Tag: NN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nisha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "'''POS Tagging\n",
    "POS using NTLK\n",
    "POS using Spacy'''\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tag import pos_tag\n",
    "doc=word_tokenize(\"What is the step by step\")\n",
    "for i in range(len(doc)):\n",
    "    print(\"Word:\",pos_tag(doc)[i][0],\"||\",\"POS Tag:\",pos_tag(doc)[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CC: Coordinating conjunction\n",
    "CD: Cardinal number\n",
    "DT: Determiner\n",
    "EX: Existential there\n",
    "FW: Foreign word\n",
    "IN: Preposition or subordinating conjunction\n",
    "JJ: Adjective\n",
    "JJR: Adjective, comparative\n",
    "JJS: Adjective, superlative\n",
    "LS: List item marker\n",
    "MD: Modal\n",
    "NN: Noun, singular or mass\n",
    "NNS: Noun, plural\n",
    "NNP: Proper noun, singular\n",
    "NNPS: Proper noun, plural\n",
    "PDT: Predeterminer\n",
    "POS: Possessive ending\n",
    "PRP: Personal pronoun\n",
    "PRP$: Possessive pronoun\n",
    "RB: Adverb\n",
    "RBR: Adverb, comparative\n",
    "RBS: Adverb, superlative\n",
    "RP: Particle\n",
    "SYM: Symbol\n",
    "TO: to\n",
    "UH: Interjection\n",
    "VB: Verb, base form\n",
    "VBD: Verb, past tense\n",
    "VBG: Verb, gerund or present participle\n",
    "VBN: Verb, past participle\n",
    "VBP: Verb, non-3rd person \n",
    "singular present\n",
    "VBZ: Verb, 3rd person singular present\n",
    "WDT: Wh-determiner\n",
    "WP: Wh-pronoun\n",
    "WP$: Possessive wh-pronoun\n",
    "WRB: Wh-adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word What || POS PRON || POS Tag:WP || Explanation pronoun\n",
      "Word is || POS AUX || POS Tag:VBZ || Explanation auxiliary\n",
      "Word the || POS DET || POS Tag:DT || Explanation determiner\n",
      "Word step || POS NOUN || POS Tag:NN || Explanation noun\n",
      "Word by || POS ADP || POS Tag:IN || Explanation adposition\n",
      "Word step || POS NOUN || POS Tag:NN || Explanation noun\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc=nlp(\"What is the step by step\")\n",
    "for word in doc:\n",
    "    print(f'''Word {word.text} || POS {word.pos_} || POS Tag:{word.tag_} || Explanation {spacy.explain(word.pos_)}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word What || POS PRON || POS Tag: WP || Explanation pronoun\n",
      "Word is || POS AUX || POS Tag: VBZ || Explanation auxiliary\n",
      "Word the || POS DET || POS Tag: DT || Explanation determiner\n",
      "Word step || POS NOUN || POS Tag: NN || Explanation noun\n",
      "Word by || POS ADP || POS Tag: IN || Explanation adposition\n",
      "Word step || POS NOUN || POS Tag: NN || Explanation noun\n",
      "Word guide || POS NOUN || POS Tag: NN || Explanation noun\n",
      "Word to || POS PART || POS Tag: TO || Explanation particle\n",
      "Word invest || POS VERB || POS Tag: VB || Explanation verb\n",
      "Word in || POS ADP || POS Tag: IN || Explanation adposition\n",
      "Word share || POS NOUN || POS Tag: NN || Explanation noun\n",
      "Word market || POS NOUN || POS Tag: NN || Explanation noun\n",
      "Word in || POS ADP || POS Tag: IN || Explanation adposition\n",
      "Word india || POS PROPN || POS Tag: NNP || Explanation proper noun\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "doc=nlp(\"What is the step by step guide to invest in share market in india\")\n",
    "for word in doc:\n",
    "    print(f'''Word {word.text} || POS {word.pos_} || POS Tag: {word.tag_} || Explanation {spacy.explain(word.pos_)}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\nisha\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('name', 'NN')\n",
      "('nisha', 'RB')\n",
      "('working', 'VBG')\n",
      "('infosys', 'NN')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\nisha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "from nltk.corpus import stopwords\n",
    "s = stopwords.words(\"english\")\n",
    "se=\"my name is nisha i am working in infosys\"\n",
    "t = [i for i in word_tokenize(se) if i not in s ]\n",
    "entities = nltk.ne_chunk(nltk.pos_tag(t))\n",
    "for entity in entities:\n",
    "    if hasattr(entity,'label'):\n",
    "        print(f'''Entity:{\" \".join(c[0] for c in entity)} || Type: {entity.label()}''')\n",
    "    else:\n",
    "        print(entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PERSON: Denotes names of people.\n",
    "ORG: Denotes names of organizations.\n",
    "LOC: Denotes locations, such as countries, cities, or mountains.\n",
    "DATE: Denotes specific dates or date ranges.\n",
    "TIME: Denotes specific times or time ranges.\n",
    "MONEY: Denotes monetary values, including currency symbols.\n",
    "PERCENT: Denotes percentage values.\n",
    "FAC: Denotes facilities, such as buildings or airports.\n",
    "GPE: Denotes geopolitical entities, such as countries, cities, or states.\n",
    "PRODUCT: Denotes names of products, such as cars or smartphones.\n",
    "EVENT: Denotes named events, such as sports events or conferences.\n",
    "WORK_OF_ART: Denotes titles of books, songs, or other works of art.\n",
    "LAW: Denotes legal references, such as laws or regulations.\n",
    "LANGUAGE: Denotes names of languages.\n",
    "NORP: Denotes nationalities or religious or political groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023 DATE\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "tokens = [i for i in word_tokenize(se) if i not in s]\n",
    "se = '''my name is shukla i m working infosys as software developer since 2023'''\n",
    "doc=nlp(se)\n",
    "for i in doc.ents:\n",
    "    print(i.text,i.label_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date : 27/04/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of word(Count Vectorizer): It is a collection of words represent a sentence with word count.\n",
    "Step inbolved in this process are clean text, tookenize ,build vocabulary, and generate vectors.\n",
    "We can create vocabulory of size 1 to n using uni-ngram ,bigram ,n-gram\n",
    "\n",
    "Advantages:\n",
    "     Simple produce and easy to implement\n",
    "     Easy to understand\n",
    "\n",
    "Disadvantages:\n",
    "       Does not consider the symmentic meaning of words\n",
    "       Due to large vector size computational time is high\n",
    "       Count vectorizer generates Spare matrix\n",
    "       Out of Vocabulary words are not captured\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of word matrix\n",
      "  (0, 8)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 1)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 5)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 4)\t1\n",
      "  (3, 8)\t1\n",
      "  (3, 3)\t1\n",
      "  (3, 6)\t1\n",
      "  (3, 2)\t1\n",
      "  (3, 1)\t1\n",
      "\n",
      " Vocabulary\n",
      "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#sample document\n",
    "document = [\"This is the first document\",\n",
    "            \"This is the second document\",\n",
    "            \"And this is the third one\",\n",
    "            \"Is this the first document\"]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "bow_matrix=vectorizer.fit_transform(document)\n",
    "vacabulary= vectorizer.get_feature_names_out()\n",
    "print(\"Bag of word matrix\")\n",
    "print(bow_matrix)\n",
    "print(\"\\n Vocabulary\")\n",
    "print(vacabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF(Term Frequency - Inverse Document Frequency): It is a statistical method.It measure how important a term or word is within a docment or sentence relative to a collection of documents or corpus.Words within a text docment are transformed into importance number by a text vectorization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix:\n",
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]\n",
      " [0.         0.42796959 0.         0.34989318 0.         0.67049706\n",
      "  0.34989318 0.         0.34989318]\n",
      " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
      "  0.26710379 0.51184851 0.26710379]\n",
      " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n",
      "\n",
      " Vocabulary\n",
      "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#sample document\n",
    "document = [\"This is the first document\",\n",
    "            \"This is the second document\",\n",
    "            \"And this is the third one\",\n",
    "            \"Is this the first document\"]\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(document)\n",
    "vacabulary=tfidf_vectorizer.get_feature_names_out()\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print(tfidf_matrix.toarray())\n",
    "print(\"\\n Vocabulary\")\n",
    "print(vacabulary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date:28/04/2024\n",
    "\n",
    "Acts and section recommendation based on crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from ntlk.tokenize import word_tokenize\n",
    "from ntlk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
